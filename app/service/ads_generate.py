import logging
import io
import os
from dotenv import load_dotenv
import requests
from openai import OpenAI
from PIL import Image, ImageDraw, ImageFont, ImageEnhance
from io import BytesIO
import base64
import re
import time
from runwayml import RunwayML
import anthropic
from moviepy import *
import uuid
from google import genai
from google.genai import types
import subprocess

logger = logging.getLogger(__name__)
load_dotenv()

# OpenAI API í‚¤ ì„¤ì •
api_key = os.getenv("GPT_KEY")
client = OpenAI(api_key=api_key)


# ë¬¸êµ¬ ìƒì„±
def generate_content(
    prompt, gpt_role, detail_content
):
    
    # gpt ì˜ì—­
    gpt_content = gpt_role
    content = prompt + '\në‚´ìš© : ' + detail_content
    client = OpenAI(api_key=os.getenv("GPT_KEY"))
    completion = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": gpt_content},
            {"role": "user", "content": content},
        ],
    )
    report = completion.choices[0].message.content
    return report

# ì‹  ë¬¸êµ¬ ìƒì„±
def generate_new_content(
    prompt
):
    # gpt ì˜ì—­
    content = prompt
    client = OpenAI(api_key=os.getenv("GPT_KEY"))
    completion = client.chat.completions.create(
        model="o1-preview",
        messages=[
            {"role": "user", "content": content}
        ],
    )
    report = completion.choices[0].message.content
    # print(report)
    return report


# êµ¬ ë¬¸êµ¬ ìƒì„±
def generate_old_content(
    prompt
):
    # gpt ì˜ì—­
    content = prompt
    client = OpenAI(api_key=os.getenv("GPT_KEY"))
    completion = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "user", "content": content},
        ],
    )
    report = completion.choices[0].message.content
    return report

# í´ë¡œë“œ ë¬¸êµ¬ ìƒì„±
def generate_claude_content(
    prompt
):
    api_key = os.getenv("CLAUDE_KEY")
    client = anthropic.Anthropic(api_key=api_key)

    result_text = ""
    response = client.messages.create(
        model="claude-3-opus-20240229",
        max_tokens=1000,
        temperature=0.0,
        system="Respond only in Korean.",
        messages=[{"role": "user", "content": prompt}]
    )
    
    if not response.content or not isinstance(response.content, list):
        result_text = "No response or unexpected response format."
    else:
        response_texts = [block.text for block in response.content if hasattr(block, 'text')]
        result_text = " ".join(response_texts)
 
    return result_text



# OpenAI API í‚¤ ì„¤ì •
api_key = os.getenv("GPT_KEY")
client = OpenAI(api_key=api_key)

# ë¯¸ë“œì €ë‹ˆ ì´ë¯¸ì§€ ìƒì„±
def generate_image_mid(
    use_option, ai_prompt
):
    use_option_propt_map = {
        'ë¬¸ìë©”ì‹œì§€': (9, 16),
        'ìœ íŠœë¸Œ ì¸ë„¤ì¼': (16, 9),
        'ì¸ìŠ¤íƒ€ê·¸ë¨ ìŠ¤í† ë¦¬': (1024, 1792),
        'ì¸ìŠ¤íƒ€ê·¸ë¨ í”¼ë“œ': (1, 1),
        'ë°°ë„ˆ': (16, 9),
        'ë„¤ì´ë²„ ë¸”ë¡œê·¸': (9, 16)
    }
    # gpt ì˜ì—­
    gpt_content = """
        You are now a Midjourney prompt engineer.
        Midjourney AI creates images based on given prompts.
        Please refer to the link below for Midjourney.
        https://en.wikipedia.org/wiki/Midjourney
        All you have to do is configure the prompt so Midjourney can generate the best image for what you're requesting.
    """
    content = ai_prompt
    client = OpenAI(api_key=os.getenv("GPT_KEY"))
    completion = client.chat.completions.create(
        model="gpt-4o",
        temperature=0.7,
        messages=[
            {"role": "system", "content": gpt_content},
            {"role": "user", "content": f"Translate the following into English and create a MidJourney prompt: {content}"},
        ],
    )
    prompt_re = completion.choices[0].message.content 

    # Get aspect ratio from use_option
    aspect_ratio = use_option_propt_map.get(use_option)
    if not aspect_ratio:
        raise ValueError("Invalid `use_option` provided.")

    # Format --ar string dynamically
    ar_str = f"--ar {aspect_ratio[0]}:{aspect_ratio[1]}"
    # style_str = f"artgerm"
    # without_text = f"without text"

    # Combine prompt with aspect ratio
    # prompt = f"{prompt_re} {without_text} {style_str} {ar_str}"
    prompt = f"{prompt_re} {ar_str}"

    USE_API_TOKEN = os.getenv("USE_API_TOKEN")
    DIS_USE_TOKEN = os.getenv("DIS_USE_TOKEN")
    DIS_SER_ID = os.getenv("DIS_SER_ID")
    DIS_CHA_ID = os.getenv("DIS_CHA_ID")

    apiUrl = "https://api.useapi.net/v2/jobs/imagine"
    token = USE_API_TOKEN
    discord = DIS_USE_TOKEN
    server = DIS_SER_ID
    channel = DIS_CHA_ID
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {token}"
    }
    body = {
        "prompt": f"{prompt}",
        "discord": f"{discord}",
        "server": f"{server}",
        "channel": f"{channel}"
    }
    response = requests.post(apiUrl, headers=headers, json=body)
    if response.status_code != 200:
        return {"error": f"Job creation failed with status {response.status_code}"}

    job_id = response.json().get("jobid")
    if not job_id:
        return {"error": "No job ID returned from API"}
    print(job_id)
    # Step 2: Polling to check job status
    apiUrl = f"https://api.useapi.net/v2/jobs/?jobid={job_id}"
    while True:
        response = requests.get(apiUrl, headers=headers)
        if response.status_code != 200:
            return {"error": f"Job status check failed with status {response.status_code}"}
        
        data = response.json()
        status = data.get("status")
        if status == "completed":
            break  # ì‘ì—… ì™„ë£Œ
        elif status in ["failed", "canceled"]:
            return {"error": f"Job failed or canceled with status: {status}"}
        
        # ì‘ì—…ì´ ì™„ë£Œë˜ì§€ ì•Šì€ ê²½ìš° ëŒ€ê¸° í›„ ì¬ìš”ì²­
        print("Job not ready yet, retrying in 5 seconds...")
        time.sleep(5)  # 5ì´ˆ ëŒ€ê¸°

    link = data.get("attachments")

    if link and len(link) > 0:
        url = link[0]['proxy_url']  # ì´ë¯¸ì§€ URL ê°€ì ¸ì˜¤ê¸°

        # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
        image_response = requests.get(url)
        if image_response.status_code == 200:
            image = Image.open(BytesIO(image_response.content))
            width, height = image.size

            # ìë¥¼ ì¢Œí‘œ ì„¤ì •
            coordinates = [
                (0, 0, width // 2, height // 2),       # ì™¼ìª½ ìƒë‹¨
                (width // 2, 0, width, height // 2),   # ì˜¤ë¥¸ìª½ ìƒë‹¨
                (0, height // 2, width // 2, height),  # ì™¼ìª½ í•˜ë‹¨
                (width // 2, height // 2, width, height)  # ì˜¤ë¥¸ìª½ í•˜ë‹¨
            ]

            # ì˜ë¦° ì´ë¯¸ì§€ ê°ì²´ ë¦¬ìŠ¤íŠ¸ ìƒì„±
            img_parts = []
            for coord in coordinates:
                cropped_image = image.crop(coord)  # ìë¥¸ ì´ë¯¸ì§€
                img_parts.append(cropped_image)  # ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€

            # ì˜ë¦° ì´ë¯¸ì§€ ê°ì²´ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
            return {"images": img_parts}
        else:
            return {"error": "Failed to download image from proxy URL"}
    else:
        return {"error": "No attachments found in job response"}


# IMAGEN3 ì´ë¯¸ì§€ ìƒì„±
def generate_image_imagen3(use_option, ai_prompt):
    try:
        size_mapping = {
            'ë¬¸ìë©”ì‹œì§€': "9:16",
            'ìœ íŠœë¸Œ ì¸ë„¤ì¼': "16:9",
            'ì¸ìŠ¤íƒ€ê·¸ë¨ ìŠ¤í† ë¦¬': "9:16",
            'ì¸ìŠ¤íƒ€ê·¸ë¨ í”¼ë“œ': "1:1",
            'ë°°ë„ˆ': "16:9",
            'ë„¤ì´ë²„ ë¸”ë¡œê·¸': "9:16"
        }
        size = size_mapping.get(use_option, "1024x1024")

        key = os.getenv("IMAGEN3_API_SECRET")
        client = genai.Client(api_key=key)

        # Prompt ì „ë‹¬ ë° ì´ë¯¸ì§€ ìƒì„±
        response = client.models.generate_images(
            model='imagen-3.0-generate-002',
            prompt=ai_prompt,
            
            config=types.GenerateImagesConfig(
                number_of_images=1,
                aspect_ratio=size,
                output_mime_type='image/jpeg'
            )
        )
        # ì´ë¯¸ì§€ ì—´ê¸°
        img_parts = []
        for generated_image in response.generated_images:
            image = Image.open(BytesIO(generated_image.image.image_bytes))
            img_parts.append(image)

        return img_parts

    except Exception as e:
        return {"error": f"ì´ë¯¸ì§€ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"}


# ì´ë¯¸ì§€ ìƒì„±
def generate_image(
    use_option, korean_image_prompt
):
    
    # gpt ì˜ì—­
    gpt_content = """
        ë‹¹ì‹ ì€ ì „ë¬¸ ë²ˆì—­ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ìê°€ ì œê³µí•œ ë‚´ìš©ì„ ì •í™•íˆ ì˜ì–´ë¡œ ë²ˆì—­í•˜ì„¸ìš”. ë²ˆì—­ ì™¸ì˜ ë¶€ê°€ì ì¸ ì„¤ëª…ì´ë‚˜ ì¶”ê°€ì ì¸ ë‚´ìš©ì„ ì‘ì„±í•˜ì§€ ë§ˆì„¸ìš”.
    """    
    content = korean_image_prompt
    client = OpenAI(api_key=os.getenv("GPT_KEY"))
    completion = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": gpt_content},
            {"role": "user", "content": content},
        ],
    )
    english_image_prompt = completion.choices[0].message.content
    # token = os.getenv("FACE_KEY")
    # if model_option == 'basic':
    #     # print(prompt)
    #     API_URL = "https://api-inference.huggingface.co/models/stabilityai/stable-diffusion-3.5-large"
    #     headers = {"Authorization": f"Bearer {token}"}
    #     data = {"inputs": prompt}
    #     # API ìš”ì²­ ë³´ë‚´ê¸°
    #     try:
    #         # API ìš”ì²­ ë³´ë‚´ê¸°
    #         response = requests.post(API_URL, headers=headers, json=data)
    #         response.raise_for_status()  # ì—ëŸ¬ ë°œìƒ ì‹œ ì˜ˆì™¸ ì²˜ë¦¬

    #         # ì‘ë‹µ ë°”ì´ë„ˆë¦¬ ë°ì´í„°ë¥¼ PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜
    #         image = Image.open(BytesIO(response.content))

    #         if resize:
    #             target_width = resize[0]  # ì›í•˜ëŠ” ê°€ë¡œ í¬ê¸°
    #             original_width, original_height = image.size
    #             aspect_ratio = original_height / original_width  # ì„¸ë¡œ/ê°€ë¡œ ë¹„ìœ¨ ê³„ì‚°

    #             # ìƒˆë¡œìš´ ì„¸ë¡œ í¬ê¸° ê³„ì‚°
    #             target_height = int(target_width * aspect_ratio)

    #             # ë¦¬ì‚¬ì´ì¦ˆ ìˆ˜í–‰
    #             image = image.resize((target_width, target_height), Image.LANCZOS)

    #         # ì´ë¯¸ì§€ë¥¼ Base64ë¡œ ì¸ì½”ë”©
    #         buffered = BytesIO()
    #         image.save(buffered, format="PNG")  # PNG í˜•ì‹ìœ¼ë¡œ ì €ì¥
    #         img_str = base64.b64encode(buffered.getvalue()).decode("utf-8")
    #         image_list.append(f"data:image/png;base64,{img_str}")
    #         # JSON í˜•ì‹ìœ¼ë¡œ Base64 ì´ë¯¸ì§€ ë°˜í™˜
    #         return {"image": image_list}

    #     except requests.exceptions.RequestException as e:
    #         print(f"Failed to generate image: {e}")
    #         return {"error": str(e)}
        
    try:
        # Resizeì™€ Final Size ë§¤í•‘
        resize_mapping = {
            'ì¹´ì¹´ì˜¤í†¡': (1024, 1792),
            'ìœ íŠœë¸Œ ì¸ë„¤ì¼': (1792, 1024),
            'ì¸ìŠ¤íƒ€ê·¸ë¨ ìŠ¤í† ë¦¬': (1024, 1792),
            'ì¸ìŠ¤íƒ€ê·¸ë¨ í”¼ë“œ': (1024, 1024),
            'ë„¤ì´ë²„ ë¸”ë¡œê·¸': (1792, 1024),
            'ë¬¸ìë©”ì‹œì§€': (1024, 1792)
        }
        resize = resize_mapping.get(use_option, None)

        if not resize:
            raise ValueError("Invalid `use_option` provided or no resize option available.")

        resize_str = f"{resize[0]}x{resize[1]}"
  
        # Prompt ì „ë‹¬ ë° ì´ë¯¸ì§€ ìƒì„±
        response = client.images.generate(
            model="dall-e-3",
            prompt=english_image_prompt,
            size=resize_str,
            quality="hd", 
            n=1
        )

        image_url = response.data[0].url
        # print(image_url)
        # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
        image_response = requests.get(image_url)
        image_response.raise_for_status()

        # ì´ë¯¸ì§€ ì—´ê¸°
        img = Image.open(io.BytesIO(image_response.content))

        return [img]
        
    except Exception as e:
        return {"error": f"ì´ë¯¸ì§€ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"}


# ì˜ìƒ ìƒì„±
def generate_video(file_path):
    ROOT_PATH = os.getenv("ROOT_PATH")
    AUDIO_PATH = os.getenv("AUDIO_PATH")
    full_audio_path = os.path.join(ROOT_PATH, AUDIO_PATH.strip("/"), "audio.mp3")

    api_key = os.getenv("RUNWAYML_API_SECRET")

    if not api_key:
        raise ValueError("API key not found. Please check your .env file.")

    # Initialize the RunwayML client with the API key
    client = RunwayML(api_key=api_key)

    image_path = file_path

    # Encode image to base64
    with open(image_path, "rb") as f:
        base64_image = base64.b64encode(f.read()).decode("utf-8")

    try : 
        # Create a new image-to-video task
        task = client.image_to_video.create(
            model='gen3a_turbo',
            prompt_image=f"data:image/png;base64,{base64_image}",
            prompt_text='Make the subject of the image move vividly.',
        )
        task_id = task.id
    except Exception as e:
        print(e)

    # Poll the task until it's complete
    print("Task created. Polling for status...")
    while True:
        task = client.tasks.retrieve(task_id)
        if task.status in ['SUCCEEDED', 'FAILED']:
            break
        print(f"Task status: {task.status}. Retrying in 10 seconds...")
        time.sleep(10)

    # Check final status
    if task.status == 'SUCCEEDED':
        if os.path.exists(file_path):
            os.remove(file_path)
        result_url = task.output[0]  # output is a list, so take the first element

        video_file_path = "output.mp4"
        response = requests.get(result_url, stream=True)

        with open(video_file_path, "wb") as f:
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)

        audio_path = full_audio_path

        VIDEO_PATH = os.getenv("VIDEO_PATH")
        output_path = os.path.join(ROOT_PATH, VIDEO_PATH.strip("/"), "video_with_audio.mp4")
        return_path = os.path.join(VIDEO_PATH, "video_with_audio.mp4")
        client_path = return_path.replace("/app", "")
        # output_path = "video_with_audio.mp4"
        video = VideoFileClip(video_file_path)
        audio = AudioFileClip(audio_path).subclipped(0, video.duration)
        video_with_audio = video.with_audio(audio)
        video_with_audio.write_videofile(output_path, codec="libx264", audio_codec="aac")

        # Close resources
        video.close()
        audio.close()
        video_with_audio.close()

        return {"result_url": client_path}
    else:
        print("Task failed.")
        # Log failure details
        print(f"Failure Reason: {task.failure}")
        print(f"Failure Code: {task.failure_code}")


# ì˜ìƒ ë¬¸êµ¬ í•©ì¹˜ê¸°
def generate_add_text_to_video(video_path, text):
    root_path = os.getenv("ROOT_PATH", ".")
    
    video_path = video_path.get('result_url')
    video_path = video_path.lstrip("/").replace("\\", "/")
    video_path = os.path.join(root_path, "app", video_path)

    # âœ… ë””ë²„ê¹…: íŒŒì¼ ê²½ë¡œ í™•ì¸
    print(f"ğŸ“‚ ìµœì¢… video_path: {video_path}")
    if not os.path.exists(video_path):
        raise FileNotFoundError(f"ğŸš¨ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {video_path}")

    # âœ… FFmpegë¡œ ë¹„ë””ì˜¤ íŒŒì¼ í™•ì¸
    result = subprocess.run(["ffmpeg", "-i", video_path], capture_output=True, text=True)
    print(result.stderr)  # FFmpeg ì˜¤ë¥˜ ë¡œê·¸ ì¶œë ¥

    # Load the video clip
    clip = VideoFileClip(video_path)
    clip = VideoFileClip(video_path).subclipped(0, clip.duration - 0.1)
    # í°íŠ¸ ê²½ë¡œ ì²˜ë¦¬
    font = os.path.join(root_path, "app", "static", "font", "Pretendard-Bold.ttf") 

    # âœ… ë””ë²„ê¹…: í°íŠ¸ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    print(f"ğŸ¨ í°íŠ¸ ê²½ë¡œ: {font}")
    if not os.path.exists(font):
        raise FileNotFoundError(f"ğŸš¨ í°íŠ¸ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {font}")

    # Create a text clip
    txt_clip = TextClip(
        font=font,
        text=text,
        font_size=56,
        color="#fff",
        text_align="center",
    )

    txt_clip = txt_clip.with_duration(clip.duration).with_position('center')

    # Composite the text clip onto the video clip
    result = CompositeVideoClip([clip, txt_clip])
    clip.close()
    txt_clip.close()

    exist_video_path = os.getenv("VIDEO_PATH", "/app/static/video")
    save_path = os.path.join(root_path, exist_video_path.lstrip("/"), "video_with_text.mp4")

    # âœ… ì €ì¥ ê²½ë¡œ ë””ë ‰í† ë¦¬ ì²´í¬ í›„ ìƒì„±
    save_dir = os.path.dirname(save_path)
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)

    # Export the result to a file
    result.write_videofile(save_path)

    # ì—…ë¡œë“œ ì„±ê³µ í›„ íŒŒì¼ ì‚­ì œ
    if os.path.exists(video_path):
        os.remove(video_path)

    return {"result_url": save_path}